## Heap & Graph Hashmap
1. Recursion
   - 自上向下：先办事在call，往往call到leaf node的null终止，同时从root到leaf的path上，办事所得到的result要带着往下call，往往return的是void
   - 自下向上：先call后办事，往往是那种需要知道子树信息才能得到当前node的相关知识时，自下向上得到最下面的信息，自下向上返值
2. O(n)时间把n size问题转换成一个n/2 size问题，时间复杂度O(n)，e.g. quick selection
3. O(1)时间，把n size问题劈成两个n/2 size问题，时间复杂度O(n)，e.g. merge sort中的divide
4. Heap
   - Size: 决定heap各种操作的时间复杂度
   - Priority: 大小比较的基准，改写comparator function
   - 使用priorityqueue实现
   - 两个结构：
     - 逻辑结构：complete binary tree
     - 存储结构：unsorted array，即上面的complete binary tree进行level order traverse之后得到的array
   - 6个性质：
     - heap order：
       - Min heap, Max heap，上下有序
       - 对于every node，每个node是以该node为根节点的subtree的最大/小值
       - 因此判断是否是heap order，只要比较当前node和左右left和right的val，因为left和right是各个子树的最大/小值
     - complete binary tree:
       - level order traverse的过程中不会出现null
       - 如果已知当前node在存储结构array中的index，可以O(1)时间得到左右child和其parent的index
     - given an element with index x in the unsorted array
       - parent: (x - 1)/2
       - left child: 2x + 1
       - right child: 2x + 2
     - Heap只能access top node when using it，也就是API来源于queue，只有peek, offer, poll
     - Available API for CRUD operation:
       - insert(put/offer/add): 把新的值插在最后，由于不符合heap逻辑结构，要进行precolate up操作，即和parent相比较，不符合上下有序，则parent和插入值交换，同时继续和再上一层的parent比较看是否符合上下有序，因此最worst情况是一路交换到root，一共logn层，所以时间复杂度是O(logn)
       - get, O(1) 只能get heap顶部
       - update：在heap中，无法update任意位置的val！只能是删掉顶部的，在插入新的，这个过程即为poll+insert，所以也是O(logn)，而这个过程并不常用，所以在priorityqueue没有类似的update
       - delete(pop/poll/remove): 由于priorityqueue性质，heap只支持删除顶部操作，因此整个poll的过程是：
         - get出顶部的element；
         - 把存储结构的最后的element的val赋值到顶部的val
         - 把存储结构中的最后element删除
         - 然后对顶部新的root做precolate down，如果和左右child不符合上下有序，把左右child相对更符合（max heap即为左右child比较大的那个，min heap则是左右child比较小的那个）的和当前交换
         - 交换到新的地方继续和新的下面的左右child进行比较，以此类推
         - 由于最worst情况也是直接换到最下层，也是logn的时间复杂度
     - Heapify：把一个unsorted array，变成符合heap逻辑结构的unsorted array
   - heap在java中的实现```Queue<Student> heap = new PriorityQueue<>();```
     - 如果不传入comparator function:
       - 默认是Min heap！要用Max heap也要传入改写的comparator
       - 如果heap里放的是object，比较是基于reference地址
       - 如果是int，则是基于int的大小比较
     - comparator vs comparable：
       - 以heap举例，heap中放入object
         - 每个object有默认的comparable function，即地址，heap不传入任何comparator的话基于比较的就是reference地址。object可以override comparable function，也就是自己定义什么是“大”，什么是“小”
         - comparator是个object，可以把user defined的comparator传入到heap中（或者别的要自定义比较的class中），然后comparator有个compare function，是传入两个object，return 一个int， 1或者-1说明这两个object哪个大哪个小（return 0则是相等）。所以我们要override这个compare function来实现自定义。通常要把三种情况即什么时候return -1，1，和0都要写，有时候当是user defined一个max heap，直接就是return value2 - value1，对应于正负情况即可
5. Hashmap, hashtable, hashset区别
   - hashset只放一个unique key，hashmap和hashtable放的是key value pair
   - hashtable线程安全，hashmap非线程安全
   - hashtable有synchronize加锁机制保证线程安全，加锁在大量access的情况下会很影响速度，所以一般用hashmap
   - hashset:
     - 使用Set interface声明，HashSet创建，```Set<Integer> set = new HashSet<>();```
     - ```set.contains(key)``` 判断set中是否含有当前key
     - ```set.add(key)```：return boolean, if addable return true and __add it to the hashset__; if not addable return false。兼具判断hashset中是否存在和不存在加入的两个功能
     - ```set.remove(key)```
     - 上述三个crud操作时间复杂度都是O(1)
     - 关于Hashset的object查重：
      ```java
      int[] array1 = new int[] {1, 2}
      int[] array2 = new int[] {1, 2}
      set.add(array1), set.add(array2)

      List<Integer> list1 = new ArrayList<>(); {1,2}
      List<Integer> list2 = new ArrayList<>(); {1,2}
      set.add(list1), set.add(list2)
      ```
     - 上述一个array，一个list，当add array时，hashset比较的是两个array的reference，所以尽管两个array都是{1, 2}，但是内存地址不一样，所以都能add进去；而当add的是两个list时，由于list这个改写了comparable的compare函数，也就是比较的不是默认的地址，而是list的具体内容，所以第二个{1, 2}是加不进去的
   - HashMap:
     - 使用map interface声明，使用HashMap进行创建，```Map<key, value> map = new HashMap<>();```
     - key必须是unique，value不一定，所以一般说一一对应，其实也是多一对应（多unique key对应相同的value）
     - 只能表示key 到 value单向的对应关系（类似数学中的function），无法表示双向对应关系
     - ```map.containsKey(key)```（同时还有个```map.containsValue(value)```，不用是因为时间复杂度高）。```map.containsKey(key)```的时间复杂度是O(1)，直接通过hash function找到
     - ```map.put(key, value)```，兼具create和update功能，如果没有key，则create这个key value pair，同时return null；如果存在相同的key，则把这个key的valueupdate成输入的value，把覆盖掉的那个原始的value return出来
     - ```map.get(key)```把这个key所对应的value拿出来，如果没有这个key，则return null，注意这里无法区别是key不存在的情况和这个key本身对应那个value就是null的情况。同理，这里null本身也可以作为hashmap的key
     - ```map.remove(key)```
     - 这里上面这4个API的时间复杂度都是O(1)
     - hashmap可以一多对应关系，key对应一个list of value，一个list of object也是可以的
     - 关于改写已经存在hashmap中value的问题：
       - 注意如果是改写hashmap中key对应的value，如果改写的是value本身，需要把它get出来，改写好在put进去
       - 如果改写的是value这个object中的field，可以get然后dereference其field，直接改写其field就可以
       - e.g. map存的是int，```map.get(key) += 1;```；map存的是object```map.get(key).age += 1```。后者可以，前者不行！因为前面get出来的是Integer，这里Integer是primitive type （int）所对应的wrapper是immutable，所以加一之后存到别的内存地址，而由于没有重新put回去，所以map中的value没有更新！
     - 限时即焚，有个key value pair，给定时间，例如10分钟，10分钟可以通过key get到value，10分钟过了就get不到了
       - ```hashmap <key, object(string, expireTime)>```，value是个wrap string内容和expireTime的一个object
         - 第一次get，把expire time改成距离当前时间10分钟后
         - 每次get看expiretime，如果没超过，则继续get
         - 超过了，不能get，同时把这个key value pair删除
       - followup可以是只能get一次，即阅后即焚；也可以是get一次时间刷新，即延时即焚，只要距离上次get不到10分钟，则依然可以get，否则不行
         - 阅后即焚：第一次get之后直接删除这个key value pair
         - 延时即焚: 也就是每次get出来put回去时候把expire time更新下即可
## Q1 Kth smallest to largest element in an unsorted array (L215)
1. Description
   - 找到Kth smallest这个值
2. Clarification
   - kth smallest?如果定义大小比较
3. Follow up
   - Q1.1 Smallest K elements in unsorted array。输出可以要求sorted和unsorted
     - S1，输出前k直接就是sorted
     - S2，输出前k不是sorted，要sorted话，就花费klogk再sort一次，这里时间花费是constant，only depend on k
     - S3，poll出的就是sort好的前k
     - S4，loop完，按顺序poll出Max heap中的所有就是sort好的前k
### S1
1. Ideas：
   - 调用Arrays.sort
### S2
1. Ideas：
   - Quick selection: Quick partition + binary search 思想
     - 类似quick sort中的partition，选一个pivot，小的全放pivot左边，大的全丢到pivot右边
     - 由于找第k小，如果pivot正好位于index k-1，则此时的pivot就是第k小的
     - 如果pivot的index大于k-1，说明k-1在pivot左边，则在左半边选pivot，以此类推
     - 如果pivot的index小于k-1，说明k-1在pivot右边，则在右半边选pivot，以此类推
2. Comments:
   - 类似quick sort中的partition，只不过每次都类似binary search只保留一边再继续，这个过程就是quick selection。
   - 时间复杂度：
     - 最好情况：即一开始就选对pivot，即选到的pivot位于k-1，但是由于要知道pivot在k-1，我们需要遍历左半边和右半边才能知道位于k-1，因此是O(n)
     - 最差情况：找第1个小的，结果每次都找到是剩下的最大的，每次都遍历所有做swap，即: n + n-1 + n-2 + ... + 1 ~ O(n2)
     - 平均情况：即每次选的pivot比较好，都选到了中间，只留一半化为把n size问题转换成一个n/2 size问题，时间复杂度O(n)
### S3
1. Ideas：
   - 使用n size的Min heap
     - 把所有element放进去，不断地poll k个，第k个即为想要的数
2. Comments:
   - 如何把n size的element一次性加入？
     - 一个一个加入：nlogn，n个logn次的insert
     - heapify the whole input array：O(n)
   - poll k 个，花费k个logn的poll操作
   - 时间复杂度：n+klogn=O(n)
   - 空间复杂度：O(n)，n size的heap
### S4
1. Ideas：
   - 使用k size的Max heap
     - heap中存有所有array中当前最小的k个element
     - 来了第n个数，丢到heap中，这k+1个最大的被挤出来，heap中依然有这遍历过n size中最小的k个
2. Comments:
   - 过程：
     - heapify first first k element，O(k)
     - 把剩下的以loop的方式一个个往heap里放，如果大于heap top的数，不放；如果小于heap top的数，把top poll出来，把这个数丢进去。由于剩下n-k个，如果运气不好所有都要进入这个k size的heap，每次procolate down要logk，所以是（n-k)logk
   - 时间复杂度：k + (n-k)logk = O(n)
   - 空间复杂度：O(n)
   - 这几个Solution相比，哪个好？
     - 当k接近n
       - S3时间近似nlogn，S4近似n，S4好
       - 空间复杂度：S2最好
     - 当n很大，或者streaming flow
       - 只能用S4做
### S5
1. Ideas：
   - 使用TreeSet，要clarify整个element没有重复才可以使用TreeSet
## Q2 top K frequency words in an Dictionary
1. Description
   - null
2. Clarification
   - 分三步：
     - 统计每个word的frequency
     - 对frequency进行sort，把前k个frequency找出来
     - 把找出来的frequency对应的word return
3. Follow up
   - null
### S1
1. Ideas：
   - Hashmap + min heap + object
   - hashmap用于统计
   - heap用于frequency排序
   - object则wrap frequency和word，用于在最后一步通过frequency定位到word
2. Comments:
   - 声明这个heap要改写override comparator中的compare function
   - 如何遍历hashmap？
     - loop所有的key：```for (Key key: map.keySet())```
     - loop所有的key value pair：```for (Entry entry: map.entrySet())```
     - 这里for loop中可以使用```:```进行遍历的class object都是implement iterator和其中的iterable
### S2
1. Ideas：
   - clarify frequency是否有重复则可以使用TreeSet和TreeMap代替heap做
### S3
1. Ideas：
   - 使用trie
2. Comments:
   - 当input特别大，hashmap放不下，可以使用trie
   - 单词长度有限，trie才能放得下
   - 更大的话，分布式的trie，把各个subtree放在不同的机器上
     - 对于整个trie的过程中，各个element的reference的内存地址对应于不同机器的内存地址
   - 当分布式内存放不下，看看一个硬盘放得下？
   - 放不下就是多个分布式硬盘，即map reduce

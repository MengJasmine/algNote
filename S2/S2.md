# S2 Sorting Algorithm (v1)
## General Notes
1. 拿到各个数据结构的长度，arr.length，str.length(),list.size()，这里arr的length是field，其他是method
2. lc出现过的sorting：
   - bubble sort, __selection sort__, __mergeSort__, __quickSort__, Insertion sort, __count sort (bucket sort)__, heap sort, radix sort, __Topological Sorting__
3. sort本质就是大小比较，或者是priority的比较，也可能是customized的东西（例如基于frequency，或者先后出现次序）
4. call sort算法时，如果要customized的priority的比较关系，要传入comparator
5. bubble sort （很简单吗
6. Insertion sort （一个一个插入，见S1
7. selection sort
   - 双指针站肩问题
   - 不断地选择剩下一堆的最小值，加入到sort好的array中
   - 如何找剩下的最小值？
     - 直接无脑一个一个扫过去
     - 使用heap
     - 无重复的情况可以使用treeset和treemap
   - 具体过程：
     - 双指针i j，i初始化在i0，j初始化在i1，还有个minIndex记录当前最小值的index，初始为i，也就是0
     - j从i1开始，和i比较，
       - j小的话把minIndex设为j，j++
       - 不小则j++
     - j走完了，用minIndex那个值和i交换，i++，然后minIndex初始为i，j为i+1
     - 注意这里j设为i+1，出界问题
       - i走到最后一个数字（length-1），不用做了，所以i实际上是只要走到倒数第二个就行，也就是[0,nums.length - 2]，这样j(i+1)不会出界
   - 时间复杂度 = n-1 + n-2 + n-3 + ... + 2 + 1 = O(n^2)
     - 代码角度：for loop里for loop，n^2
   - 空间复杂度：没有用额外的数据结构，只用了pointer，所以是O(1)
   - selection sort的output可以一个int[]，也可以是void，都是作用于array本身
8. Merge sort
   - 一劈两半，问两边的sort结果，然后merge起来，局部有序到全局有序
   - 一劈两半，左右是个subarray，如何表示？直接用边界l和r可以表示，直接作用于原始array
   - divide and conquer思想，先call到base case，然后merge起来向上返值
   - dfs，call的时候进入系统栈，recursion的系统栈也可以算作空间复杂度（需要clarify），是logn（不断一劈两半，logn层，因为有且只有一条一路call到base case的路径/栈，最多logn，到底就开始pop了）
   - merge two sorted array
     - S1: create 新的array，两个pointer在两个sorted subarray的开头，谁小取谁放到新的array中
     - S2: inplace操作，其中一个subarray是放得下另一个的（这个要clarify！），放的下的那个subarray是这样的 1 2 3 4 * * * *，*是null
       - 不能正向做，会overwrite，所以发生这种overwrite情况可以尝试pointer从尾巴往前 （当然这个取决于*在尾巴还是头部，在头部也是从头开始才不会overwrite）
       - cur pointer从能放的下的array的尾部（1 2 3 4 * * * *）出发，谁大取谁，不会发生overwrite
     - S3: 注意在mergesort里，我们是用index标明两个要merge 的subarray，所以并不是两个独立的array，而是一个大array的左右两部分要merge起来 array = int[]{0 2 4 6 | 1 3 5 7}
       - deep copy 原来的array作为一个helper array，两个指针从劈成两个部分的开头走，谁小取谁放到原来的array中（这里两个指针一个在0，一个在1）
       - 这里while loop直到这两个指针有一个越界，如果是左边越界，那么右边多的就在原位，不用做postprocessing，如果右边越界，那么还要把deep copy中的左边指针剩下的copy到原始array的剩下部分
       - 时间复杂度：2n
   - stable的sort，注意重复状态也要保证次序，像是：1a 1b 1c变换后依旧要是1a 1b 1c，不能是1b 1a 1c
   - 时间复杂度：
     - divide，左右O(1)劈成两半，
       - 第一层 O(1=2^0) 劈成两半
       - 第二层 O(2=2^1) 两份劈成4半
       - 第三层 O(4=2^2) 4份劈成8半
       - ...
       - 第logn层，O(2^(logn-1)) = O(n/2)
       - 所有层加起来，1 + 2 + 4 + ... + n/2 = n - 1 = O(n)
     - conquer，同divide过程，只不过反过来，所以
       - 常见的错误时间复杂度算法
         - 所有层merge加起来，n/2 + n/4 + ... + 2 + 1 = n - 1
         - 做一次merge two是2n
         - 这里不能说：一共有n-1次merge，做一次merge two 是2n，所以总的时间复杂度是O(n^2)
         - 注意，这里每次merge 的n的size是变化的，也就是最底层是2个merge，最顶层才是n/2与n/2进行merge
       - 正确的说法是每次层merge花费n，一共logn层，所以一共花费nlogn
         - 这里每层不管你是两两merge，还是4个4个merge，你做merge two都要把所有元素扫一遍，越底层，你每次扫的少，但是有很多组要merge；越高层，你每次扫的多，但是merge的case少
     - 所以总的时间复杂度是O(n+nlogn)=O(nlogn)
   - 空间复杂度：
     - divide：O(1)
     - merge:
       - 系统栈算吗？
       - 如果每次merge到一个新的list向上return，那么每层有n size的list，一共logn层，所以一共占用nlogn的空间
         - 注意这里有java的garbage collection机制，也就是你merge完左半边，左半边创建的新的list已经被回收了，所以实际上空间占用率不会到nlogn
       - global使用一个deep copy原始array的一个helper array，使用这个占O(n)空间的helper array做所有的merge
9. Quick sort
    - 过程
      - 随机从中选择一个数字作为pivot，然后和最后一个数字swap
      - 这样pivot在整个要sort的数组最后一个
      - 从第0个和pivot前面那个（倒数第二个）开始走，two pointer（l和r）开始从两边向中间扫
      - l从左向右扫，比pivot小的l++，比pivot大的停下来
      - r从右向左扫，比pivot大的r--，比pivot小的停下来
      - l和r都停下来时，l和r进行swap
      - 然后pivot和l停下来的数字进行swap
      - 此时pivot的左边并不sort，右边同理
      - 左右半边继续call上面的选择pivot进行quick sort
    - base case是只有一个数字
    - 这里随机选pivot，同理也可以每次都直接选最左边或者最右边的值，这样就不用在开头和结尾交换pivot了
    - 为何要把pivot丢掉队尾（队头同理）？
      - 因为不想让l和r两边向中间走的时候检测到pivot从而影响与pivot的比较关系
    - 不stable的sort
    - l和r左右向中间走，while loop的跳出条件？
      - l和r相邻是不可以的，因为此时l和r还没比较，当然也可以postprocessing，不是很推荐
      - l和r相等，此时，这个相等时所在的数字并没有被check，可能比pivot大或者小，如果是大，和队尾的pivot作swap并没有什么问题，但是如果小交换了那么pivot右边就出现一个比pivot小的数字了，同理pivot放在队首的时候就是pivot大就有问题了
      - l和r相互越过，此时所有数字都被check过了，此时越过的l和r分别在比pivot大的右半边和比pivot小的左半边，所以队尾的pivot可以安心和l进行交换（同理，队首版的pivot可以和r进行交换）
      - 所以跳出条件是l和r左右越过
    - check过程中的三段区间，
      - [0, l): 这段是小于pivot的（l跳过或者被swap过来）
      - [l, r]：还没有被check或者跳出时的状态，所以当跳出时，l和r左右越过，尽管此时l跳到比pivot大的区域，算是被check过了，但是不是在小于pivot的区域，同理r也是
      - (r, length - 1 - 1]: 这段是大于pivot的（r跳过或者被swap过来）
    - 先partition，在call左右的quicksort，先全局有序，再局部有序
    - 在pivot从队尾（或队头）与l（或者r）交换，__这个位置就是pivot最终sort的位置__，因为左边一定比pivot小，右边一定比pivot大，这个属性可以用来推测从小到大第k个（机quick selection）
    - 如果有重复，可以把与pivot重复的丢到右边（或是左边），尽管右边重复的数不紧邻此时交换回来的pivot，但是之后在右边的quicksort可以使得所有重复的贴近pivot，此时的右边区间(r, length - 1 - 1]: 这段是大于等于pivot的
    - 时间复杂度：
      - 选择pivot，O(1)swap到队尾
      - l和r从左右向中间走直到左右越过，O(1)swap，一共扫了n-1，一共花费O(n)
      - 把pivot从队尾swap回来，O(1)
      - 再对左右两边分别做以上三步
      - 由于左右分出的size取决于pivot选择的好坏：
        - 好的能够以1/2均分数组，所以第一次扫n，第二次两个n/2，第三次4个n/4，...，到第logn次的n个O(1)。每次都是O(n)，一共logn次，所以总的时间复杂度是O(nlogn)
        - 如果每次pivot都选的很差，即如果每次都选的是最大或者最小值，partition出来的一边总是n-1，另一边为0，使得第一次扫n，第二次扫n-1，第三次n-2，...，第n次扫1，所以总的时间复杂度= n + n-1 + n-2 + ... + 1 = O(n2)
    - 所以关键是pivot的选择！所以一般是随机选几个，在这几个中取median，这样（经算法导论中证明）能够保证时间复杂度O(nlogn)
    - 空间复杂度 O(1)，系统stack的空间占用：average O(log(n)) worst O(n)
    - __以上所有都是基于l和r从左右向中间走，那么能不能用slow和fast指针从左边走？__
    - s和f从左向右做quicksort
      - 依旧三个区间
        - [0, s) 小于pivot
        - [s, f) 大于pivot （同理也可以处理相等情况）
        - [f, length - 1 -1]
      - 过程:
        - 随机选个pivot，和队尾交换（当然和队头交换就得s和f指针从尾向头走了）
        - 首先初始s和f在当前partition的第一个数
        - s先动，比pivot小，s++，直到比pivot大，停下来，注意除非是一开始，s不可能先于f出界，因为右边总有比pivot大的数，所以总会停下来让f走，此时f就有可能会出界
        - s停下来时，f从s开始走，比pivot大，f++，直到比pivot小，或者出界
        - 当s和f都没有出界，且都停下来（说明s找到比pivot大的，f找到比pivot小的），此时，s和f进行交换
        - 交换完，s++，继续之前步骤
        - 这里推荐使用for loop，for loop跳出条件是直到f出界，f为何会出界？此时s要扫到比pivot大时，f站在s肩上往右看找pivot小的，但是直到最后（最后指的是队尾pivot前面那个数）都没有扫到，其实此时f出界的位置就在队尾的pivot，说明什么？说明右边都是比pivot大，而此时s站的位置就是第一个比pivot大的那个，所以可以直接s和f交换，就把pivot交换回到正确的sort的位置了
    - 所有数只有两种情况，e.g. 0 1 1 0 0 1 1 0
      - partition two：l和r左右往中间走，l找1，r找0，找到即swap，直接通过是0还是1比较关系，不用选择个pivot
    - 所有数有3种情况，e.g. 0 1 2 1 0 0 2 1 1 0 2 2
      - 先partition two：把0当左边，1和2当右边
      - 再1和2这个部分做partition two：把1放左边，2放右边
    - 所有数有有限个情况k，e.g. a b c d e f
      - 分组不断做partition two
      - 以上所有有限个情况的时间复杂度都是O(n)，第一遍n，第二遍小于n的数，加了等于不加
      - 这里还可以用count sort，同理也是O(n)时间复杂度
    - stable vs unstable
10. count/bucket sort
    - 在input没有特点的情况下，时间复杂度不会超过nlogn
    - 这里的count/bucket sort可以应用在input中每个元素是有限种可能性的情况下
    - count sort：一个指针从左向右，数每个元素出现的次数，时间复杂度O(n)
    - 输出不一定是一个array，也可以是count每个frequency的hashmap，想要啥在改成啥
    - bucket sort：不计frequency，直接把元素放到每个bucket里，即：
      - 数组：1 3 5 2 2 4 3 5 1 2
      - count sort: 
        - count1: 2
        - count2: 3
        - count3: 2
        - count4: 1
        - count5: 2
      - bucket sort:
        - bucket1: 1 1
        - bucket2: 2 2 2
        - bucket3: 3 3
        - bucket4: 4
        - bucket5: 5 5
        - 每个bucket的长度动态变化，用list
11. Radix sort
  - partition ten的quick sort，即不分两边，直接分成十份，基于0-9，数字从高位向地位：
    - e.g. 123 567 167 234 890 120
      - 第一次，先看百位，根据0-9分十份，即：123 167 120 | 234 | 567 | 890
      - 第二次，再看十位，根据0-9继续分十份，即：123 120 | 167 | 234 | 567 | 890
      - ...
  - 这里partition ten可以推广到tree，也就是binary tree推广到B+ tree
12. Heap sort/sort with heap
  - Heap sort：一次又一次的heapify
  - sort with heap: 把所有值丢到minheap里，然后一个一个往外pop，每次logn做heap，一共pop出n个，时间复杂度nlogn，空间占用一个n size的heap，这个和heap sort有区别
13. topological sort
  - DAG, directed acyclic graph，有向无环图
  - 先check cycle，没有cycle才可以topological sort
14. Master theory for time complexity
  - $T(n) = a·T(n/b) + O(n^k)$
  - Then the Big-Oh of the time-complexity is thus:
    - if $a > b^k$, then the time-complexity is $O(n^{log_ba})$
    - if $a = b^k$, then the time-complexity is $O(n^k·log(n))$
    - if $a < b^k$, then the time-complexity is $O(n^k)$
    - 把一个n size的问题，以n^k的时间复杂度，化成a个n/b size的问题
  - e.g. quick sort，一个n size quicksort 问题，通过O(n)partition时间复杂度，化成2个 n/2的子问题，即 T(n) = 2·T(n/2)，即a = 2, b = 2, k = 1，所以$a = b^k$，所以$O(n^1·log(n))$
 + O(n)
15. Recursion
  - call 自己的function
  - 要有终止条件，也就是base case，base case要写全，同时要return，否则就会stack overflow
  - stack overflow 只会出现在recursion吗？并不局限于recursion，function a调用b，b调用c，c在调用a，没有合适的终止条件也会stack overflow
  - 思想：大问题分解成一个个小问题，然后通过小问题的答案来解决大问题，这里可以不用管每个小问题具体怎么解决的，只要能解决：通过小问题的答案来解决大问题，这个过程即可
  - 这里大问题分解成一个个小问题，可以是linear reduction，n到n-1再到n-2，也可以binary reduction
  - 同时，binary reduction时，是只要其中一半呢（binary search），还是左右两半都要（也就是divide and conquer）？
16. dynamic programming
  - induction，从小到大，从base case推广到n
  - 数学归纳法
  - 状态转移方程：dp[k] = f(dp[k-1], dp[k-2])，不一定只有k-1和k-2
  - dp vs recursion：
    - base case都是一样的
    - recursion从大到小，大问题转换成若干个小问题
    - dp则是通过知道一个一个的小问题来一步步转换得出大问题
    - 这上面两个转换是同一个状态转移方程，唯一区别是方向
    - recursion是大问题转成小问题，也就是有的大问题算过得小问题，在别的大问题中没法利用起来，除非加入计划存储
    - 而dp则不一样，从小到大的话，这里所有小问题都有算过，所以每一个大问题都可以通过存好的小问题直接得到，当然是以消耗一部分空间为代价，一般是值得的
    - recursion优化方向是建立计划存储，把一个个小问题的答案都存起来，新的大问题先看看是不是有的小问题已经有解决了，直接就能拿到答案
    - dp主要的优化方向还是空间复杂度，一般是mxn空间的dp存储，可以转换成一个array加一个变量，甚至可以再转换成一个变量，再就是可以inplace操作，主要思路是观察状态转移方程，比如只跟前面两个有关系，那么dp存储只保留最近两个就行了，不用n个都存
17. 如何选择test case？
  - input是int：-1 0 1 2 Integer.MIN_VALUE, Integer.MAX_VALUE
  - int array: null {} {1} {1 2} {2 1}
```java
public int[] selectSort(int[] array) {
// public void selectSort(int[] array) {
	if (array == null || array.length <= 1) return array; // c.c.要check到1，毕竟1个也不用sort了

	int minIndex;
	for (int i = 0; i < array.length - 1; i++) {	// 不需要走到最后一位
		int minIndex = i; // 初始化为当前的i，所以j从i+1开始，尽量避免Integer.MAX_VALUE, MIN_VALUE，避免之后出现+1-1越界问题
		for (int j = i + 1; j < array.length; j++) { //站在i的肩膀上用j扫，从i+1开始
			if (array[j] < array[minIndex]) minIndex = j;
		}
		swap(array, i, minIndex);
        // 这里下面两种swap都可以，因为array操作都是在其本身，不cache出return的结果array也已经发生swap了
	}
	return array;
}

private void swap(int[] array, int i, int j) {
	int temp = array[i];
	array[i] = array[j];
	array[j] = temp;
// return; 对于void可加可不加，出现在最后可以省略
}
// vs 上下两种不同写法
private int[] swap(int[] array, int i, int j) {
	int temp = array[i];
	array[i] = array[j];
	array[j] = temp;
	return array; // 不能省略
}

```
```java
private ArrayList<Integer> divideAndMerge(ArrayList<Integer> array, int left, int right) {
    ArrayList<Integer> result = new ArrayList<Integer>();
    if (left == right) {
        result.add(array.get(left));
        return result;
    }
    int mid = left + (right - left) / 2;

    ArrayList<Integer> leftResult = divideAndMerge(array, left, mid);
    // wall 只有上面return出来才会继续执行下面的语句
    ArrayList<Integer> rightResult = divideAndMerge(array, mid + 1, right);
    //
    return merge(leftResult, rightResult);
}

private void divideAndMerge( int[] nums, int start, int end, int[] helper){ // need a helper array

	if ( nums == null || nums.length <= 1 ) return; // left right to check!!!
		int n = end - start + 1;
		if( n <= 1) return;
		int mid = start + ( end - start ) / 2;
		divideAndMerge( nums, start, mid);
		divideAndMerge( nums, mid + 1, end);
		merge( nums, start, mid, end);
}
```
## Q1 Climbing stairs (L70)
1. Description
   - 从0往n层走，每次可走1步或者2步，问一共有多少种走法
2. Clarification
   - null
3. Follow up
   - Fibonacci Sequence
     - 同样recursion和dp两种想法
     - 这里增长的速度很快，有可能output出了int的边界，可以用long，或者TODO long
     - 这里n是int，条件发射可能为0或者负数，corner case 可以return 不在有效值的范围即-1，可以throw exception
### S1
1. Ideas：
   - recursion
   - 问n层有几种走法，不知道，那问n-1层和n-2层有多少种走法，当这两个小问题知道了，那n层就是n-1的走法加上n-2的走法（因为可跨1或2步，能跨3步的话就还得加上n-3的走法）
   - base case：第一层和第二层，分别有1种和2种走法
2. Comments:
   - 时间复杂度：
     - 最底部，n，开始recursion call
     - 第二层，n-1，call 2个
     - 第三层，n-2, call 4个
     - ...
     - 最顶层，1，call 2n个
     - 1 + 2 + 4 + .. + 2n = O(2^n)
   - 有冗余计算，也就是每次call一个都要call到base case才能得到值，尽管历史上已经算过了，但没法reuse
   - 可以计划存储，也就是从base case开始，从小到大不断计算一个又一个直到n，这个过程就是dp
3. Code
```java
class Solution {
    public int fib(int n) { //TODO long
      if (n < 0) throw new IllegalArgumentException();
      if (n = 0 || n = 1) {
        return n;
      }
      return fib(n-2) + fib(n-1);
   }
}
```
### S2
1. Ideas：
   - dp
   - 扎扎实实从0开始往上走
   - base case：第一层和第二层，分别有1种和2种走法
   - 第三层有几种？就是第一层的走法乘以2（其中一种是一步一步走，另一种是2步一下子走），和第二层的走法乘以1（只走一步的走法）
   - 第四层同理，看第二和第三层
   - 走走走一直到第n层，n-2的走法乘以2加上n-1的走法乘以1，就是最后答案
2. Comments:
   - 可以填一个n size 的array，也就是每到一个小问题都填一下，空间复杂度就是O(n)
   - 注意这里fib的状态转移方程中n只取决于前面两个，所以不用存n个，只要两个变量就行，空间复杂度就是O(1)了
3. Code
```java
class Solution {
    public int solution(int[] nums) {

   }
}
```
## Q2 Implement MyPow(x, n)
1. Description
   - 输出x的n次方的结果
   - linear reduction
2. Clarification
   - n可正可负可为零
3. Follow up
   - null
### S1
1. Ideas：
   - brutal force，n个x相乘
2. Comments:
   - 用while loop走一遍
   - 也可以用recursion，即f(n) = x * f(n-1)
   - O(n)
   - 有冗余计算
### S2
1. Ideas：
   - recursion binary reduction
2. Comments:
   - MyPow(x, n) --> MyPow(x, n/2) * MyPow(x, n - n/2)
   - 要区奇偶，这里要n - n/2
   - 时间复杂度，O(n)
     - 底层，开始recursion call
     - call出两个，O(1) return，这call出的两个尽管很像，偶数时甚至一致，但是由于没有计划存储，只能再次分别call下去，这里就是冗余计算，这里冗余甚至都是同层直接调用，根据奇偶来看是否要多乘以一个x，见S3
     - 两个call出4个
     - ...
     - 第logn层，call出2^logn个
     - 加起来一共O(n)个
3. Code
```java
class Solution {
  public int myPow(int x, int n) { //TODO long
    if (n == 0) return 1;
    if (n == 1) return x;
    return myPow(x, n/2) * myPow(x, n - n/2);
  }
}
```
### S3
1. Ideas：
   - recursion binary reduction with memorization
2. Comments:
   - 每次只call一边，另一边通过奇偶决定
   - 这里就是用temp来存储
   - 这里等于每次只看一半，所以是binary reduction，一共logn层，每次都是O(1)来return值
   - 所以时间复杂度是O(logn)
3. Code
```java
class Solution {
  public long myPow(int x, int n) {
    if (n == 0) return 1;
    long temp = myPow(x, n/2);
    return n % 2 == 0? temp * temp: temp * temp *x;
  }
}
```